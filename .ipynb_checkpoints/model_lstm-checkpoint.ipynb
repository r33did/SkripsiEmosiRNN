{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88811f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics as st\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d64bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1n = []\n",
    "data2n = []\n",
    "root = 'Filtered'\n",
    "emosi = ['kaget','marah','santai','senang']\n",
    "maindirs = 'IAPS hari 2'\n",
    "dirs = os.listdir(maindirs)\n",
    "def lowpass_filter(sinyal,fcl):\n",
    "    sampleRate = 50\n",
    "    wnl = fcl/(sampleRate)\n",
    "    b,a = signal.butter(3,wnl,'lowpass')\n",
    "    fil = signal.filtfilt(b, a, sinyal)\n",
    "    return fil\n",
    "\n",
    "def filtering(folder):\n",
    "    print(\"Filter dimulai, harap tunggu sebentar\")\n",
    "    dirs = os.listdir(folder)\n",
    "    for j in dirs:\n",
    "        df = pd.read_csv(folder+'/'+str(j))\n",
    "        print(j)\n",
    "        #wk = df[\"Waktu\"]\n",
    "        pp = df['Pipi']\n",
    "        al = df['Alis']\n",
    "        #wkt = list(wk)\n",
    "        data1 = list(pp)\n",
    "        data2 = list(al)\n",
    "\n",
    "        t = [i for i in range(len(data1))]\n",
    "        w = lowpass_filter(data1,2.0)\n",
    "        x = lowpass_filter(data2,2.0)\n",
    "\n",
    "        mn1 = min(w)\n",
    "        mx1 = max(w)\n",
    "        mn2 = min(x)\n",
    "        mx2 = max(x)\n",
    "\n",
    "        for i in range(len(w)):\n",
    "            data1n.append((w[i]-mn1)/(mx1-mn1))\n",
    "            data2n.append((x[i]-mn2)/(mx2-mn2))\n",
    "\n",
    "        f = plt.figure()\n",
    "        plt.xlabel('Data ke-')\n",
    "        plt.ylabel('mV')\n",
    "        plt.grid(True)\n",
    "        plt.title(j)\n",
    "        plt.plot(t,data1n)\n",
    "        plt.plot(t,data2n)\n",
    "        plt.savefig('Data_Plot3/'+j+'2.png')\n",
    "        f.clear()\n",
    "        plt.close(f)\n",
    "        d_t = list(zip(data1n,data2n))\n",
    "        root = 'Data_filter4'\n",
    "        finaldirs = os.path.join(root,j)\n",
    "        df1 = pd.DataFrame(d_t,columns=['Pipi','Alis'])\n",
    "        df1.to_csv(finaldirs)\n",
    "        data1n.clear()\n",
    "        data2n.clear()\n",
    "    print('Filter Selesai !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fc559cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter dimulai, harap tunggu sebentar\n",
      "aditNH1.csv\n",
      "aditNH2.csv\n",
      "aditNH3.csv\n",
      "aditNH4.csv\n",
      "aditNH5.csv\n",
      "aditNL1.csv\n",
      "aditNL2.csv\n",
      "aditNL3.csv\n",
      "aditNL4.csv\n",
      "aditNL5.csv\n",
      "aditPH1.csv\n",
      "aditPH2.csv\n",
      "aditPH3.csv\n",
      "aditPH4.csv\n",
      "aditPH5.csv\n",
      "aditPL1.csv\n",
      "aditPL2.csv\n",
      "aditPL3.csv\n",
      "aditPL4.csv\n",
      "aditPL5.csv\n",
      "agusNH1.csv\n",
      "agusNH2.csv\n",
      "agusNH3.csv\n",
      "agusNH4.csv\n",
      "agusNH5.csv\n",
      "agusNL1.csv\n",
      "agusNL2.csv\n",
      "agusNL3.csv\n",
      "agusNL4.csv\n",
      "agusNL5.csv\n",
      "agusPH1.csv\n",
      "agusPH2.csv\n",
      "agusPH3.csv\n",
      "agusPH4.csv\n",
      "agusPH5.csv\n",
      "agusPL1.csv\n",
      "agusPL2.csv\n",
      "agusPL3.csv\n",
      "agusPL4.csv\n",
      "agusPL5.csv\n",
      "aminNH1.csv\n",
      "aminNH2.csv\n",
      "aminNH3.csv\n",
      "aminNH4.csv\n",
      "aminNH5.csv\n",
      "aminNL1.csv\n",
      "aminNL2.csv\n",
      "aminNL3.csv\n",
      "aminNL4.csv\n",
      "aminNL5.csv\n",
      "aminPH1.csv\n",
      "aminPH2.csv\n",
      "aminPH3.csv\n",
      "aminPH4.csv\n",
      "aminPH5.csv\n",
      "aminPL1.csv\n",
      "aminPL2.csv\n",
      "aminPL3.csv\n",
      "aminPL4.csv\n",
      "aminPL5.csv\n",
      "bagusNH1.csv\n",
      "bagusNH2.csv\n",
      "bagusNH3.csv\n",
      "bagusNH4.csv\n",
      "bagusNH5.csv\n",
      "bagusNL1.csv\n",
      "bagusNL2.csv\n",
      "bagusNL3.csv\n",
      "bagusNL4.csv\n",
      "bagusNL5.csv\n",
      "bagusPH1.csv\n",
      "bagusPH2.csv\n",
      "bagusPH3.csv\n",
      "bagusPH4.csv\n",
      "bagusPH5.csv\n",
      "bagusPL1.csv\n",
      "bagusPL2.csv\n",
      "bagusPL3.csv\n",
      "bagusPL4.csv\n",
      "bagusPL5.csv\n",
      "basithNH1.csv\n",
      "basithNH2.csv\n",
      "basithNH3.csv\n",
      "basithNH4.csv\n",
      "basithNH5.csv\n",
      "basithNL1.csv\n",
      "basithNL2.csv\n",
      "basithNL3.csv\n",
      "basithNL4.csv\n",
      "basithNL5.csv\n",
      "basithPH1.csv\n",
      "basithPH2.csv\n",
      "basithPH3.csv\n",
      "basithPH4.csv\n",
      "basithPH5.csv\n",
      "basithPL1.csv\n",
      "basithPL2.csv\n",
      "basithPL3.csv\n",
      "basithPL4.csv\n",
      "basithPL5.csv\n",
      "ekaNH1.csv\n",
      "ekaNH2.csv\n",
      "ekaNH3.csv\n",
      "ekaNH4.csv\n",
      "ekaNH5.csv\n",
      "ekaNL1.csv\n",
      "ekaNL2.csv\n",
      "ekaNL3.csv\n",
      "ekaNL4.csv\n",
      "ekaNL5.csv\n",
      "ekaPH1.csv\n",
      "ekaPH2.csv\n",
      "ekaPH3.csv\n",
      "ekaPH4.csv\n",
      "ekaPH5.csv\n",
      "ekaPL1.csv\n",
      "ekaPL2.csv\n",
      "ekaPL3.csv\n",
      "ekaPL4.csv\n",
      "ekaPL5.csv\n",
      "hanifNH1.csv\n",
      "hanifNH2.csv\n",
      "hanifNH3.csv\n",
      "hanifNH4.csv\n",
      "hanifNH5.csv\n",
      "hanifNL1.csv\n",
      "hanifNL2.csv\n",
      "hanifNL3.csv\n",
      "hanifNL4.csv\n",
      "hanifNL5.csv\n",
      "hanifPH1.csv\n",
      "hanifPH2.csv\n",
      "hanifPH3.csv\n",
      "hanifPH4.csv\n",
      "hanifPH5.csv\n",
      "hanifPL1.csv\n",
      "hanifPL2.csv\n",
      "hanifPL3.csv\n",
      "hanifPL4.csv\n",
      "hanifPL5.csv\n",
      "rizkiNH1.csv\n",
      "rizkiNH2.csv\n",
      "rizkiNH3.csv\n",
      "rizkiNH4.csv\n",
      "rizkiNH5.csv\n",
      "rizkiNL1.csv\n",
      "rizkiNL2.csv\n",
      "rizkiNL3.csv\n",
      "rizkiNL4.csv\n",
      "rizkiNL5.csv\n",
      "rizkiPH1.csv\n",
      "rizkiPH2.csv\n",
      "rizkiPH3.csv\n",
      "rizkiPH4.csv\n",
      "rizkiPH5.csv\n",
      "rizkiPL1.csv\n",
      "rizkiPL2.csv\n",
      "rizkiPL3.csv\n",
      "rizkiPL4.csv\n",
      "rizkiPL5.csv\n",
      "Filter Selesai !\n"
     ]
    }
   ],
   "source": [
    "filtering('IAPS hari 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90a2dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1n = []\n",
    "data2n = []\n",
    "root_filter = 'Filtered'\n",
    "emosi = ['NH','NL','PH','PL']\n",
    "# pasien = ['adit','agus','amin','eka','riznop']\n",
    "pasien = ['adit','agus','amin','bagus','basith','eka','hanif','rizki']\n",
    "stdvn1 = []\n",
    "rrtn1 = []\n",
    "mdn1 = []\n",
    "stdvn2 = []\n",
    "rrtn2 = []\n",
    "mdn2 = []\n",
    "emosi_list = []\n",
    "count = 0\n",
    "root_extract = 'IAPS hari 2'\n",
    "rawdata = []\n",
    "pipi = []\n",
    "alis = []\n",
    "wkt = []\n",
    "count = 0\n",
    "header_list = ['Waktu','Pipi','Alis']\n",
    "data = []\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "885b49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int((len(dirs2)-1)/4)+1\n",
    "def extract_feature(folder):\n",
    "    dirs2 = os.listdir(folder)\n",
    "    count = 0\n",
    "    root = 'IAPS3_extract'\n",
    "    for y in pasien:\n",
    "        for i in emosi:\n",
    "            for j in range(1,6):\n",
    "                df = pd.read_csv(folder+'/'+y+i+str(j)+'.csv')\n",
    "                print(folder+'/'+y+i+str(j)+'.csv')\n",
    "                data1 = list(df['Pipi'].to_numpy())\n",
    "                data2 = list(df['Alis'].to_numpy())\n",
    "                stdv1 = st.stdev(data1)\n",
    "                rrt1 = st.mean(data1)\n",
    "                md1 = st.median(data1)\n",
    "                stdvn1.append(stdv1)\n",
    "                rrtn1.append(rrt1)\n",
    "                mdn1.append(md1)\n",
    "                stdv2 = st.stdev(data2)\n",
    "                rrt2 = st.mean(data2)\n",
    "                md2 = st.median(data2)\n",
    "                stdvn2.append(stdv2)\n",
    "                rrtn2.append(rrt2)\n",
    "                mdn2.append(md2)\n",
    "                if(i == 'NH'):\n",
    "                    mk = 1\n",
    "                    emosi_list.append(mk)\n",
    "                elif(i == 'NL'):\n",
    "                    mk = 2\n",
    "                    emosi_list.append(mk)\n",
    "                elif(i == 'PH'):\n",
    "                    mk = 3\n",
    "                    emosi_list.append(mk)\n",
    "                elif(i == 'PL'):\n",
    "                    mk = 4\n",
    "                    emosi_list.append(mk)\n",
    "#                 print('Selesai !')\n",
    "    namafile = 'iaps4_extracted.csv'\n",
    "#     namafile = 'tes_extracted2.csv'\n",
    "    finaldirs = os.path.join(root,namafile)\n",
    "    df1 = pd.DataFrame({'STDEV1' : stdvn1,'AVG1' : rrtn1,'MDN1' : mdn1,\n",
    "                        'STDEV2':stdvn2,'AVG2' : rrtn2,'MDN2' : mdn2,'EMOSI' : emosi_list})\n",
    "    df1.to_csv(finaldirs,mode='w+',index=False)\n",
    "    print(finaldirs)\n",
    "    stdvn1.clear()\n",
    "    rrtn1.clear()\n",
    "    mdn1.clear()\n",
    "    stdvn2.clear()\n",
    "    rrtn2.clear()\n",
    "    mdn2.clear()\n",
    "    emosi_list.clear()\n",
    "    print('Ekstraksi Fitur Selesai !')\n",
    "            #namafile = i+'_extracted.csv'\n",
    "            #finaldirs = os.path.join(root,namafile)\n",
    "            #if(i == 'kaget'):\n",
    "            #    i = 1\n",
    "            #elif(i == 'marah'):\n",
    "            #    i = 2\n",
    "            #elif(i == 'santai'):\n",
    "            #    i = 3\n",
    "            #elif(i == 'senang'):\n",
    "            #    i = 4\n",
    "            #df1 = pd.DataFrame({'STDEV1' : stdvn1,'AVG1' : rrtn1,'MDN1' : mdn1,\n",
    "            #                    'STDEV2':stdvn2,'AVG2' : rrtn2,'MDN2' : mdn2,'EMOSI' : i})\n",
    "            #df1.to_csv(finaldirs,mode='w+')\n",
    "            #print(finaldirs)\n",
    "            #stdvn1.clear()\n",
    "            #rrtn1.clear()\n",
    "            #mdn1.clear()\n",
    "            #stdvn2.clear()\n",
    "            #rrtn2.clear()\n",
    "            #mdn2.clear()\n",
    "\n",
    "def create_model():\n",
    "    model = keras.models.Sequential([\n",
    "            keras.layers.LSTM(8, return_sequences=True, input_shape=[None,6]),\n",
    "            keras.layers.LSTM(8),\n",
    "            keras.layers.Dense(4, activation='softmax')\n",
    "            ])\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd5dc1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_filter4/aditNH1.csv\n",
      "Data_filter4/aditNH2.csv\n",
      "Data_filter4/aditNH3.csv\n",
      "Data_filter4/aditNH4.csv\n",
      "Data_filter4/aditNH5.csv\n",
      "Data_filter4/aditNL1.csv\n",
      "Data_filter4/aditNL2.csv\n",
      "Data_filter4/aditNL3.csv\n",
      "Data_filter4/aditNL4.csv\n",
      "Data_filter4/aditNL5.csv\n",
      "Data_filter4/aditPH1.csv\n",
      "Data_filter4/aditPH2.csv\n",
      "Data_filter4/aditPH3.csv\n",
      "Data_filter4/aditPH4.csv\n",
      "Data_filter4/aditPH5.csv\n",
      "Data_filter4/aditPL1.csv\n",
      "Data_filter4/aditPL2.csv\n",
      "Data_filter4/aditPL3.csv\n",
      "Data_filter4/aditPL4.csv\n",
      "Data_filter4/aditPL5.csv\n",
      "Data_filter4/agusNH1.csv\n",
      "Data_filter4/agusNH2.csv\n",
      "Data_filter4/agusNH3.csv\n",
      "Data_filter4/agusNH4.csv\n",
      "Data_filter4/agusNH5.csv\n",
      "Data_filter4/agusNL1.csv\n",
      "Data_filter4/agusNL2.csv\n",
      "Data_filter4/agusNL3.csv\n",
      "Data_filter4/agusNL4.csv\n",
      "Data_filter4/agusNL5.csv\n",
      "Data_filter4/agusPH1.csv\n",
      "Data_filter4/agusPH2.csv\n",
      "Data_filter4/agusPH3.csv\n",
      "Data_filter4/agusPH4.csv\n",
      "Data_filter4/agusPH5.csv\n",
      "Data_filter4/agusPL1.csv\n",
      "Data_filter4/agusPL2.csv\n",
      "Data_filter4/agusPL3.csv\n",
      "Data_filter4/agusPL4.csv\n",
      "Data_filter4/agusPL5.csv\n",
      "Data_filter4/aminNH1.csv\n",
      "Data_filter4/aminNH2.csv\n",
      "Data_filter4/aminNH3.csv\n",
      "Data_filter4/aminNH4.csv\n",
      "Data_filter4/aminNH5.csv\n",
      "Data_filter4/aminNL1.csv\n",
      "Data_filter4/aminNL2.csv\n",
      "Data_filter4/aminNL3.csv\n",
      "Data_filter4/aminNL4.csv\n",
      "Data_filter4/aminNL5.csv\n",
      "Data_filter4/aminPH1.csv\n",
      "Data_filter4/aminPH2.csv\n",
      "Data_filter4/aminPH3.csv\n",
      "Data_filter4/aminPH4.csv\n",
      "Data_filter4/aminPH5.csv\n",
      "Data_filter4/aminPL1.csv\n",
      "Data_filter4/aminPL2.csv\n",
      "Data_filter4/aminPL3.csv\n",
      "Data_filter4/aminPL4.csv\n",
      "Data_filter4/aminPL5.csv\n",
      "Data_filter4/bagusNH1.csv\n",
      "Data_filter4/bagusNH2.csv\n",
      "Data_filter4/bagusNH3.csv\n",
      "Data_filter4/bagusNH4.csv\n",
      "Data_filter4/bagusNH5.csv\n",
      "Data_filter4/bagusNL1.csv\n",
      "Data_filter4/bagusNL2.csv\n",
      "Data_filter4/bagusNL3.csv\n",
      "Data_filter4/bagusNL4.csv\n",
      "Data_filter4/bagusNL5.csv\n",
      "Data_filter4/bagusPH1.csv\n",
      "Data_filter4/bagusPH2.csv\n",
      "Data_filter4/bagusPH3.csv\n",
      "Data_filter4/bagusPH4.csv\n",
      "Data_filter4/bagusPH5.csv\n",
      "Data_filter4/bagusPL1.csv\n",
      "Data_filter4/bagusPL2.csv\n",
      "Data_filter4/bagusPL3.csv\n",
      "Data_filter4/bagusPL4.csv\n",
      "Data_filter4/bagusPL5.csv\n",
      "Data_filter4/basithNH1.csv\n",
      "Data_filter4/basithNH2.csv\n",
      "Data_filter4/basithNH3.csv\n",
      "Data_filter4/basithNH4.csv\n",
      "Data_filter4/basithNH5.csv\n",
      "Data_filter4/basithNL1.csv\n",
      "Data_filter4/basithNL2.csv\n",
      "Data_filter4/basithNL3.csv\n",
      "Data_filter4/basithNL4.csv\n",
      "Data_filter4/basithNL5.csv\n",
      "Data_filter4/basithPH1.csv\n",
      "Data_filter4/basithPH2.csv\n",
      "Data_filter4/basithPH3.csv\n",
      "Data_filter4/basithPH4.csv\n",
      "Data_filter4/basithPH5.csv\n",
      "Data_filter4/basithPL1.csv\n",
      "Data_filter4/basithPL2.csv\n",
      "Data_filter4/basithPL3.csv\n",
      "Data_filter4/basithPL4.csv\n",
      "Data_filter4/basithPL5.csv\n",
      "Data_filter4/ekaNH1.csv\n",
      "Data_filter4/ekaNH2.csv\n",
      "Data_filter4/ekaNH3.csv\n",
      "Data_filter4/ekaNH4.csv\n",
      "Data_filter4/ekaNH5.csv\n",
      "Data_filter4/ekaNL1.csv\n",
      "Data_filter4/ekaNL2.csv\n",
      "Data_filter4/ekaNL3.csv\n",
      "Data_filter4/ekaNL4.csv\n",
      "Data_filter4/ekaNL5.csv\n",
      "Data_filter4/ekaPH1.csv\n",
      "Data_filter4/ekaPH2.csv\n",
      "Data_filter4/ekaPH3.csv\n",
      "Data_filter4/ekaPH4.csv\n",
      "Data_filter4/ekaPH5.csv\n",
      "Data_filter4/ekaPL1.csv\n",
      "Data_filter4/ekaPL2.csv\n",
      "Data_filter4/ekaPL3.csv\n",
      "Data_filter4/ekaPL4.csv\n",
      "Data_filter4/ekaPL5.csv\n",
      "Data_filter4/hanifNH1.csv\n",
      "Data_filter4/hanifNH2.csv\n",
      "Data_filter4/hanifNH3.csv\n",
      "Data_filter4/hanifNH4.csv\n",
      "Data_filter4/hanifNH5.csv\n",
      "Data_filter4/hanifNL1.csv\n",
      "Data_filter4/hanifNL2.csv\n",
      "Data_filter4/hanifNL3.csv\n",
      "Data_filter4/hanifNL4.csv\n",
      "Data_filter4/hanifNL5.csv\n",
      "Data_filter4/hanifPH1.csv\n",
      "Data_filter4/hanifPH2.csv\n",
      "Data_filter4/hanifPH3.csv\n",
      "Data_filter4/hanifPH4.csv\n",
      "Data_filter4/hanifPH5.csv\n",
      "Data_filter4/hanifPL1.csv\n",
      "Data_filter4/hanifPL2.csv\n",
      "Data_filter4/hanifPL3.csv\n",
      "Data_filter4/hanifPL4.csv\n",
      "Data_filter4/hanifPL5.csv\n",
      "Data_filter4/rizkiNH1.csv\n",
      "Data_filter4/rizkiNH2.csv\n",
      "Data_filter4/rizkiNH3.csv\n",
      "Data_filter4/rizkiNH4.csv\n",
      "Data_filter4/rizkiNH5.csv\n",
      "Data_filter4/rizkiNL1.csv\n",
      "Data_filter4/rizkiNL2.csv\n",
      "Data_filter4/rizkiNL3.csv\n",
      "Data_filter4/rizkiNL4.csv\n",
      "Data_filter4/rizkiNL5.csv\n",
      "Data_filter4/rizkiPH1.csv\n",
      "Data_filter4/rizkiPH2.csv\n",
      "Data_filter4/rizkiPH3.csv\n",
      "Data_filter4/rizkiPH4.csv\n",
      "Data_filter4/rizkiPH5.csv\n",
      "Data_filter4/rizkiPL1.csv\n",
      "Data_filter4/rizkiPL2.csv\n",
      "Data_filter4/rizkiPL3.csv\n",
      "Data_filter4/rizkiPL4.csv\n",
      "Data_filter4/rizkiPL5.csv\n",
      "IAPS3_extract\\iaps4_extracted.csv\n",
      "Ekstraksi Fitur Selesai !\n"
     ]
    }
   ],
   "source": [
    "# extract_feature(folder='Data_filter4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fb4da6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, None, 8)           480       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 544       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 1,060\n",
      "Trainable params: 1,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "maindirs = 'IAPS3_extract'\n",
    "dirs = os.listdir(maindirs)\n",
    "emosi = ['NH','NL','PH','PL']\n",
    "df = pd.read_csv(maindirs+\"/\"+\"janai.csv\")\n",
    "d_t = df.drop('EMOSI',axis=1)\n",
    "label = pd.get_dummies(df['EMOSI'])\n",
    "data_len = int(len(d_t))\n",
    "for i in range (0,data_len):\n",
    "    temp = d_t.iloc[i]\n",
    "    temp_list = temp.values.tolist()\n",
    "    X.append(temp_list)\n",
    "for j in range(0,data_len):\n",
    "    temp1 = label.iloc[j]\n",
    "    temp1_list = temp1.values.tolist()\n",
    "    y.append(temp1_list)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "length = 420\n",
    "num_train = 278\n",
    "index = np.random.randint(0,length, size=length)\n",
    "train_X = X[index[0:num_train]]\n",
    "train_Y = y[index[0:num_train]]\n",
    "test_X = X[index[num_train:]]\n",
    "test_Y = y[index[num_train:]]\n",
    "# train_X = X[0:num_train]\n",
    "# train_Y = y[0:num_train]\n",
    "# test_X = X[num_train:]\n",
    "# test_Y = y[num_train:]\n",
    "train_X = np.reshape(train_X, (train_X.shape[0],1,train_X.shape[1]))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0],1,test_X.shape[1]))\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "history = model.fit(\n",
    "     train_X,\n",
    "     train_Y,\n",
    "     batch_size = 20,\n",
    "     epochs=30,\n",
    "     callbacks=[callback],\n",
    "     validation_data=(test_X,test_Y),\n",
    "     )\n",
    "inpoot = int(input(\"Apakah mau simpan model ? \"))\n",
    "if inpoot == 1:\n",
    "    nama_model = str(input('Nama model = '))\n",
    "    model.save(nama_model)\n",
    "    model.save_weights(nama_model+'.h5')\n",
    "    print(\"Model berhasil disimpan !\")\n",
    "    keras.backend.clear_session()\n",
    "else:\n",
    "    print(\"ga disimpen\")\n",
    "    keras.backend.clear_session()\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
